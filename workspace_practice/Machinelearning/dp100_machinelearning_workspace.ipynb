{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Machinelearning Workspace - DP-100 Practice\n",
    "\n",
    "**Workspace:** Machinelearning  \n",
    "**Resource Group:** AI-102  \n",
    "**Location:** Canada East  \n",
    "**Compute:** Standard_DS11_v2 (Running)\n",
    "\n",
    "This notebook provides hands-on practice for Azure DP-100 certification using the Machinelearning workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Module 1: Environment Setup & Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import Data, Environment, Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Machinelearning workspace\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"29f1cd2f-d0e2-413e-b913-1976b6924fa6\",\n",
    "    resource_group_name=\"AI-102\",\n",
    "    workspace_name=\"Machinelearning\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Connected to Machinelearning workspace\")\n",
    "\n",
    "# Verify connection\n",
    "workspace = ml_client.workspaces.get(\"Machinelearning\")\n",
    "print(f\"üìç Workspace: {workspace.display_name}\")\n",
    "print(f\"üìç Location: {workspace.location}\")\n",
    "print(f\"üìç Resource Group: {workspace.resource_group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available compute resources\n",
    "computes = list(ml_client.compute.list())\n",
    "print(f\"üñ•Ô∏è Available compute resources: {len(computes)}\")\n",
    "\n",
    "for compute in computes:\n",
    "    print(f\"  ‚Ä¢ {compute.name} ({compute.type})\")\n",
    "    if hasattr(compute, 'size'):\n",
    "        print(f\"    Size: {compute.size}\")\n",
    "    if hasattr(compute, 'state'):\n",
    "        print(f\"    State: {compute.state}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Module 2: Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = {\n",
    "    'age': np.random.normal(45, 15, n_samples).clip(18, 80),\n",
    "    'income': np.random.normal(50000, 20000, n_samples).clip(10000, 150000),\n",
    "    'credit_score': np.random.normal(650, 100, n_samples).clip(300, 850),\n",
    "    'debt_ratio': np.random.normal(0.3, 0.2, n_samples).clip(0, 1),\n",
    "    'employment_years': np.random.normal(10, 5, n_samples).clip(0, 40),\n",
    "    'approved': np.random.choice([0, 1], n_samples, p=[0.3, 0.7])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"‚úÖ Sample dataset created\")\n",
    "print(f\"üìä Dataset shape: {df.shape}\")\n",
    "print(f\"üéØ Target distribution: {df['approved'].value_counts().to_dict()}\")\n",
    "\n",
    "# Save locally\n",
    "df.to_csv('credit_approval_data.csv', index=False)\n",
    "print(\"üíæ Dataset saved as credit_approval_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register dataset in Azure ML\n",
    "data_asset = Data(\n",
    "    path=\"./credit_approval_data.csv\",\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Credit approval dataset for DP-100 practice\",\n",
    "    name=\"credit-approval-data\"\n",
    ")\n",
    "\n",
    "registered_data = ml_client.data.create_or_update(data_asset)\n",
    "print(\"‚úÖ Dataset registered in Azure ML\")\n",
    "print(f\"üìÅ Name: {registered_data.name}\")\n",
    "print(f\"üè∑Ô∏è Version: {registered_data.version}\")\n",
    "print(f\"üîó Path: {registered_data.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "print(\"üìà Dataset Overview:\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä Statistical Summary:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîç Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚ùì Missing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Module 3: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "X = df.drop('approved', axis=1)\n",
    "y = df['approved']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"üìä Data Split Summary:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"Train: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "lr_prob = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "\n",
    "print(\"üîç Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, lr_pred))\n",
    "print(\"\\nCoefficients:\")\n",
    "for feature, coef in zip(X.columns, lr_model.coef_[0]):\n",
    "    print(f\"{feature}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(\"üå≤ Random Forest Results:\")\n",
    "print(f\"Accuracy: {rf_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, rf_pred))\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(X.columns, rf_model.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest'],\n",
    "    'Accuracy': [lr_accuracy, rf_accuracy]\n",
    "})\n",
    "\n",
    "print(\"üèÜ Model Comparison:\")\n",
    "print(models_comparison)\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = models_comparison.loc[models_comparison['Accuracy'].idxmax(), 'Model']\n",
    "best_accuracy = models_comparison['Accuracy'].max()\n",
    "\n",
    "print(f\"\\nüéØ Best Model: {best_model_name} (Accuracy: {best_accuracy:.4f})\")\n",
    "\n",
    "# Select best model for registration\n",
    "if best_model_name == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "    model_type = 'RandomForest'\n",
    "else:\n",
    "    best_model = lr_model\n",
    "    model_type = 'LogisticRegression'\n",
    "\n",
    "print(f\"üì¶ Selected model for registration: {model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Module 4: Model Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model locally\n",
    "model_filename = f\"{model_type.lower().replace(' ', '_')}_model.pkl\"\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"üíæ Model saved locally as: {model_filename}\")\n",
    "\n",
    "# Verify file exists\n",
    "import os\n",
    "if os.path.exists(model_filename):\n",
    "    file_size = os.path.getsize(model_filename)\n",
    "    print(f\"‚úÖ File exists (Size: {file_size} bytes)\")\n",
    "else:\n",
    "    print(\"‚ùå File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register model in Azure ML\n",
    "model_asset = Model(\n",
    "    path=model_filename,\n",
    "    name=f\"dp100-{model_type.lower().replace(' ', '-')}-model\",\n",
    "    version=\"1\",\n",
    "    description=f\"{model_type} model trained for DP-100 credit approval prediction\",\n",
    "    type=AssetTypes.CUSTOM_MODEL,\n",
    "    properties={\n",
    "        \"accuracy\": str(best_accuracy),\n",
    "        \"algorithm\": model_type,\n",
    "        \"dataset\": \"credit-approval-data\",\n",
    "        \"training_samples\": str(X_train.shape[0]),\n",
    "        \"features\": str(X_train.shape[1])\n",
    "    },\n",
    "    tags={\n",
    "        \"project\": \"dp100-practice\",\n",
    "        \"workspace\": \"machinelearning\",\n",
    "        \"certification\": \"dp100\"\n",
    "    }\n",
    ")\n",
    "\n",
    "registered_model = ml_client.models.create_or_update(model_asset)\n",
    "\n",
    "print(\"‚úÖ Model registered successfully!\")\n",
    "print(f\"üè∑Ô∏è Name: {registered_model.name}\")\n",
    "print(f\"üìä Version: {registered_model.version}\")\n",
    "print(f\"üÜî ID: {registered_model.id}\")\n",
    "print(f\"üìù Description: {registered_model.description}\")\n",
    "\n",
    "if hasattr(registered_model, 'tags') and registered_model.tags:\n",
    "    print(f\"üè∑Ô∏è Tags: {registered_model.tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Module 5: Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Data Visualization\n",
    "Create visualizations to understand the credit approval dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Credit Approval Dataset Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Distribution plots\n",
    "features = ['age', 'income', 'credit_score', 'debt_ratio', 'employment_years']\n",
    "for i, feature in enumerate(features):\n",
    "    row, col = i // 3, i % 3\n",
    "    if row < 2 and col < 3:\n",
    "        sns.histplot(data=df, x=feature, ax=axes[row, col], kde=True)\n",
    "        axes[row, col].set_title(f'{feature.replace(\"_\", \" \").title()} Distribution')\n",
    "\n",
    "# Target distribution in the last subplot\n",
    "if len(features) < 6:\n",
    "    approval_counts = df['approved'].value_counts()\n",
    "    axes[1, 2].bar(['Rejected', 'Approved'], approval_counts.values, \n",
    "                   color=['#ff6b6b', '#4ecdc4'], alpha=0.7)\n",
    "    axes[1, 2].set_title('Loan Approval Distribution')\n",
    "    axes[1, 2].set_ylabel('Count')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(approval_counts.values):\n",
    "        axes[1, 2].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('credit_analysis_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved as 'credit_analysis_visualization.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Feature Engineering\n",
    "Create new features and analyze their impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "df_engineered = df.copy()\n",
    "\n",
    "# Income to debt ratio\n",
    "df_engineered['income_to_debt'] = df_engineered['income'] / (df_engineered['debt_ratio'] + 0.001)\n",
    "\n",
    "# Age groups\n",
    "df_engineered['age_group'] = pd.cut(df_engineered['age'], \n",
    "                                   bins=[0, 25, 35, 45, 55, 100], \n",
    "                                   labels=['18-25', '26-35', '36-45', '46-55', '56+'])\n",
    "\n",
    "# Credit score categories\n",
    "df_engineered['credit_category'] = pd.cut(df_engineered['credit_score'], \n",
    "                                         bins=[0, 580, 670, 740, 850], \n",
    "                                         labels=['Poor', 'Fair', 'Good', 'Excellent'])\n",
    "\n",
    "# Employment stability\n",
    "df_engineered['employment_stability'] = pd.cut(df_engineered['employment_years'], \n",
    "                                              bins=[0, 2, 5, 10, 40], \n",
    "                                              labels=['New', 'Developing', 'Stable', 'Experienced'])\n",
    "\n",
    "# Convert categorical to numeric\n",
    "df_engineered['age_group_encoded'] = df_engineered['age_group'].cat.codes\n",
    "df_engineered['credit_category_encoded'] = df_engineered['credit_category'].cat.codes\n",
    "df_engineered['employment_stability_encoded'] = df_engineered['employment_stability'].cat.codes\n",
    "\n",
    "print(\"‚úÖ New features created:\")\n",
    "print(\"‚Ä¢ income_to_debt: Income relative to debt\")\n",
    "print(\"‚Ä¢ age_group: Categorized age groups\")\n",
    "print(\"‚Ä¢ credit_category: Credit score categories\")\n",
    "print(\"‚Ä¢ employment_stability: Employment experience levels\")\n",
    "\n",
    "print(f\"\\nüìä Dataset shape after engineering: {df_engineered.shape}\")\n",
    "print(\"\\nSample of engineered features:\")\n",
    "print(df_engineered[['age', 'age_group', 'credit_score', 'credit_category', 'income_to_debt']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Model Improvement\n",
    "Train a model with the engineered features and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare engineered features\n",
    "numerical_features = ['age', 'income', 'credit_score', 'debt_ratio', 'employment_years', \n",
    "                     'income_to_debt', 'age_group_encoded', 'credit_category_encoded', \n",
    "                     'employment_stability_encoded']\n",
    "\n",
    "X_engineered = df_engineered[numerical_features]\n",
    "y_engineered = df_engineered['approved']\n",
    "\n",
    "# Split data\n",
    "X_train_eng, X_test_eng, y_train_eng, y_test_eng = train_test_split(\n",
    "    X_engineered, y_engineered, test_size=0.2, random_state=42, stratify=y_engineered\n",
    ")\n",
    "\n",
    "print(\"üîß Engineered Features Model Training:\")\n",
    "print(f\"Features used: {len(numerical_features)}\")\n",
    "print(f\"Training samples: {X_train_eng.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_eng.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with engineered features\n",
    "rf_engineered = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=12,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_engineered.fit(X_train_eng, y_train_eng)\n",
    "rf_eng_pred = rf_engineered.predict(X_test_eng)\n",
    "rf_eng_accuracy = accuracy_score(y_test_eng, rf_eng_pred)\n",
    "\n",
    "print(\"üå≤ Random Forest with Engineered Features:\")\n",
    "print(f\"Accuracy: {rf_eng_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_eng, rf_eng_pred))\n",
    "\n",
    "# Feature importance for engineered features\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': numerical_features,\n",
    "    'importance': rf_engineered.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüîç Top 5 Most Important Features:\")\n",
    "print(feature_importance_df.head())\n",
    "\n",
    "# Compare with original model\n",
    "improvement = rf_eng_accuracy - rf_accuracy\n",
    "print(f\"\\nüìà Accuracy Improvement: {improvement:.4f} ({improvement*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Certification Practice Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice completion summary\n",
    "practice_summary = {\n",
    "    \"workspace\": \"Machinelearning\",\n",
    "    \"location\": \"Canada East\",\n",
    "    \"compute\": \"Standard_DS11_v2\",\n",
    "    \"modules_completed\": [\n",
    "        \"Environment Setup\",\n",
    "        \"Data Management\",\n",
    "        \"Model Training\",\n",
    "        \"Model Registration\",\n",
    "        \"Data Visualization\",\n",
    "        \"Feature Engineering\",\n",
    "        \"Model Improvement\"\n",
    "    ],\n",
    "    \"models_registered\": 1,\n",
    "    \"datasets_created\": 1,\n",
    "    \"best_accuracy\": rf_eng_accuracy,\n",
    "    \"skills_practiced\": [\n",
    "        \"Azure ML SDK\",\n",
    "        \"Data Preparation\",\n",
    "        \"Model Training\",\n",
    "        \"Model Evaluation\",\n",
    "        \"Feature Engineering\",\n",
    "        \"Model Registration\",\n",
    "        \"Data Visualization\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üéì DP-100 Practice Summary for Machinelearning Workspace:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in practice_summary.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\nüéØ Ready for DP-100 Certification!\")\n",
    "print(\"Next: Practice with AIintern workspace or deploy models to production.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Azure ML Environment",
   "language": "python",
   "name": "azure_ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}